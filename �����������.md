# 神经网络的量化

参考：https://pytorch.apachecn.org/#/

https://zhuanlan.zhihu.com/p/144025236

https://www.w3cschool.cn/pytorch/pytorch-ildt3bxe.html

------

####  目前很多高精度的深度学习模型所需内存、计算量和能耗巨大，并不适合部署在一些低成本的嵌入式设备中，为了解决这个矛盾，模型压缩技术应运而生，其主要是通过减少原始模型参数的数量或比特数来实现对内存和计算需求的降低，从而进一步降低能耗。目前性能最稳定的就是INT8的模型量化技术，相对于原始模型的FP32计算相比，INT8量化可将模型大小减少 4 倍，并将内存带宽要求减少 4 倍，对 INT8 计算的硬件支持通常快 2 到 4 倍。 值得注意的是量化主要是一种加速前向推理的技术，并且绝大部分的量化算子仅支持前向传递。

#### 数据类型

- weight的8 bit量化 ：data_type = qint8，数据范围为[-128, 127]
- activation的8 bit量化：data_type = quint8，数据范围为[0, 255]

bias一般是不进行量化操作的，仍然保持float32的数据类型，还有一个需要提前说明的，weight在浮点模型训练收敛之后一般就已经固定住了，所以根据原始数据就可以直接量化，然而activation会因为每次输入数据的不同，导致数据范围每次都是不同的，所以针对这个问题，在量化过程中专门会有一个校准过程，即提前准备一个小的校准数据集，在测试这个校准数据集的时候会记录每一次的activation的数据范围，然后根据记录值确定一个固定的范围。

**支持后端：**

- 具有 AVX2 支持或更高版本的 x86 CPU：fbgemm
- ARM CPU：qnnpack

## **量化方法**

1. **Post Training Dynamic Quantization**：这是最简单的一种量化方法，Post Training指的是在浮点模型训练收敛之后进行量化操作，其中weight被提前量化，而activation在前向推理过程中被动态量化，即每次都要根据实际运算的浮点数据范围每一层计算一次scale和zero_point，然后进行量化；
2. **Post Training Static Quantization**：第一种不是很常见，一般说的Post Training Quantization指的其实是这种静态的方法，而且这种方法是最常用的，其中weight跟上述一样也是被提前量化好的，然后activation也会基于之前校准过程中记录下的固定的scale和zero_point进行量化，整个过程不存在量化参数*(*scale和zero_point)的再计算；
3. **Quantization Aware Training**：对于一些模型在浮点训练+量化过程中精度损失比较严重的情况，就需要进行量化感知训练，即在训练过程中模拟量化过程，数据虽然都是表示为float32，但实际的值的间隔却会受到量化参数的限制。

至于为什么不在一开始训练的时候就模拟量化操作是因为8bit精度不够容易导致模型无法收敛，甚至直接使用16bit进行from scrach的量化训练都极其容易导致无法收敛，不过目前已经有了一些tricks去缓解这个问题，但不在本文讨论之列。

## 量化流程

以最常用的Post Training (Static) Quantization为例：

1. **准备模型：**准备一个训练收敛了的浮点模型**，**用**QuantStub**和**DeQuantstub**模块指定需要进行量化的位置；

**2. 模块融合：**将一些相邻模块进行融合以提高计算效率，比如conv+relu或者conv+batch normalization+relu，最常提到的BN融合指的是conv+bn通过计算公式将bn的参数融入到weight中，并生成一个bias；

![img](https://pic3.zhimg.com/80/v2-92750fa03da1aba8e72778064d10ca16_720w.jpg)

**3. 确定量化方案：**这一步需要指定量化的后端(qnnpack/fbgemm/None)，量化的方法(per-layer/per-channel，对称/非对称)，activation校准的策略(最大最小/移动平均/**L2Norm(这个不太清楚，是类似TensorRT的校准方式吗？？？**))；

**4. activation校准：**利用torch.quantization.prepare() 插入将在校准期间观察激活张量的模块，然后将校准数据集灌入模型，利用校准策略得到每层activation的scale和zero_point并存储；

**5. 模型转换：**使用 torch.quantization.convert(）函数对整个模型进行量化的转换。 这其中包括：它量化权重，计算并存储要在每个激活张量中使用的scale和zero_point，替换关键运算符的量化实现；



https://blog.csdn.net/zlgahu/article/details/104662203/

# 深度学习模型量化（低精度推理）大总结

模型量化作为一种能够有效减少模型大小，加速深度学习推理的优化技术，已经得到了学术界和工业界的广泛研究和应用。模型量化有 8/4/2/1 bit等，本文主要讨论目前相对比较成熟的 8-bit 低精度推理。 通过这篇文章你可以学习到以下内容：1）量化算法介绍及其特点分析，让你知其然并知其所以然； 2）Pytorch 量化实战，让你不再纸上谈兵；3）模型精度及性能的调优经验分享，让你面对问题不再束手无策；4）完整的量化文献干货合集，让你全面系统地了解这门主流技术。
1.CPU 推理性能提升 2-4 倍，模型大小降低至1/4，模型量化真的这么好使？

维基百科中关于量化（quantization）的定义是: 量化是将数值 x 映射到 y 的过程，其中 x 的定义域是一个大集合(通常是连续的)，而 y 的定义域是一个小集合（通常是可数的【1】。8-bit 低精度推理中， 我们将一个原本 FP32 的 weight/activation 浮点数张量转化成一个 int8/uint8 张量来处理。模型量化会带来如下两方面的好处：

## 减少内存带宽和存储空间

深度学习模型主要是记录每个 layer（比如卷积层/全连接层） 的 weights 和 bias, FP32 模型中，每个 weight 数值原本需要 32-bit 的存储空间，量化之后只需要 8-bit 即可。因此，模型的大小将直接降为将近 1/4。

不仅模型大小明显降低， activation 采用 8-bit 之后也将明显减少对内存的使用，这也意味着低精度推理过程将明显减少内存的访问带宽需求，提高高速缓存命中率，尤其对于像 batch-norm， relu，elmentwise-sum 这种内存约束(memory bound)的 element-wise 算子来说，效果更为明显。

## 提高系统吞吐量（throughput），降低系统延时（latency）

直观理解，试想对于一个 专用寄存器宽度为 512 位的 SIMD 指令，当数据类型为 FP32 而言一条指令能一次处理 16 个数值，但是当我们采用 8-bit 表示数据时，一条指令一次可以处理 64 个数值。因此，在这种情况下，可以让芯片的理论计算峰值增加 4 倍。在CPU上，英特尔至强可扩展处理器的 AVX-512 和 VNNI 高级矢量指令支持低精度和高精度的累加操作，详情可以参考文献【2】。

## 2.量化设计

按照量化阶段的不同，一般将量化分为 quantization aware training(QAT) 和 post-training quantization(PTQ)。QAT 需要在训练阶段就对量化误差进行建模，这种方法一般能够获得较低的精度损失。PTQ 直接对普通训练后的模型进行量化，过程简单，不需要在训练阶段考虑量化问题，因此，在实际的生产环境中对部署人员的要求也较低，但是在精度上一般要稍微逊色于 QAT。本文介绍的主要方法也是针对 PTQ 。关于 QAT 的内容，因为理论较为复杂，我打算后续重新写一篇文章详细介绍。

在介绍量化算法之前，我们先看一下浮点数和 8-bit 整数的完整表示范围。

![img](https://imgconvert.csdnimg.cn/aHR0cDovL3AxLnBzdGF0cC5jb20vbGFyZ2UvcGdjLWltYWdlL2ZjZjViY2Y2YmM2ZjQ1Mzk5ZjFhN2QyOThjNWFhZGU1?x-oss-process=image/format,png)

量化算法负责将 FP32 数据映射到 int8/uint8 数据。实际的 weight/activiation 浮点数动态范围可能远远小于 FP32 的完整表示范围，为了简单起见，在下面的量化算法介绍中，我们直接选取 FP32 张量的最大值(max)和最小值(min)来说明量化算法，更为详细的实际动态范围确定方法将在后续部分说明。量化算法分为对称算法和非对称算法，下面我们主要介绍这两种算法的详细内容及其区别。

## 非对称算法 （asymmetric）

如下图所示，非对称算法那的基本思想是通过 收缩因子（scale） 和 零点（zero point） 将 FP32 张量 的 min/max 映射分别映射到 8-bit 数据的 min/max。

![img](https://imgconvert.csdnimg.cn/aHR0cDovL3AxLnBzdGF0cC5jb20vbGFyZ2UvcGdjLWltYWdlLzE1MWQwMmNkZWE5YjQ3MDk4YTRmZjI5NGQ4N2YyMzQ3?x-oss-process=image/format,png)

如果我们用 x_f 表示 原始浮点数张量, 用 x_q 表示量化张量, 用 q_x 表示 scale，用 zp_x 表示 zero_point, n 表示量化数值的 bit数，这里 n=8， 那么非对称算法的量化公式如下：

![img](https://imgconvert.csdnimg.cn/aHR0cDovL3AxLnBzdGF0cC5jb20vbGFyZ2UvZGZpYy1pbWFnZWhhbmRsZXIvNGFlYjc1ODYtNjQyZi00YmQ3LWEwMGQtY2E5ZTFkMWYyMzIz?x-oss-process=image/format,png)

上述公式中引入了 zero_point 的概念。它通常是一个整数，即 zp_x= rounding(q_x * min_x_f)。因此，在量化之后，浮点数中的 0 刚好对应这个整数。这也意味着 zero_point 可以无误差地量化浮点数中的数据 0，从而减少补零操作（比如卷积中的padding zero）在量化中产生额外的误差【3,4】。

但是，从上述公式我们可以发现 x_q 的结果只能是一个非负数，这也意味着其无法合理地处理有符号的 int8 量化，Pytorch 的处理措施是将零点向左移动 -128，并限制其在 [-128,127] 之间【5】。

